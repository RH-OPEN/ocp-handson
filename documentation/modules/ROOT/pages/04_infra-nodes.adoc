= Infra ノードの作成
:navtitle: infra-node
include::_attributes.adoc[]

本コンテンツは、link:https://github.com/team-ohc-jp-place/openshift-cns-testdrive[MAD Workshop Ops Track] の内容をベースに、一部変更しています。

== 演習の概要
OpenShiftのサブスクリプションモデルでは、顧客は追加料金なしで様々なコアインフラストラクチャコンポーネントをを実行できます。つまり、OpenShiftのコアインフラストラクチャコンポーネントのみを実行しているノードは、クラスター環境をカバーするために必要なサブスクリプションの総数にはカウントされません。

インフラストラクチャのカテゴライズに該当するOpenShiftコンポーネントは以下が含まれます。

* kubernetesとOpenShiftのコントロールプレーンサービス（"masters"）。
* ルータ
* コンテナイメージレジストリ
* クラスタメトリクスの収集 ("monitoring")
* クラスタ集約型ロギング
* サービスブローカー

上記以外のコンテナ/Pod/コンポーネントを実行しているノードはすべてワーカーとみなされ、サブスクリプションでカバーされている必要があります。

---

### MachineSet 詳細
`MachineSets` の演習では、`MachineSets` を使用して、レプリカ数を変更してクラスタをスケーリングすることを検討しました。インフラストラクチャノードの場合、特定のKubernetesラベルを持つ `Machine` を追加で作成したいと思います。そして、それらのラベルを持つノード上で特定の動作をするように様々なインフラストラクチャコンポーネントを設定することができます。

[Note]
====
現在、インフラストラクチャコンポーネントの制御に使用されているOperatorは、"taint" と "toleration" の使用をすべてサポートしているわけではありません。これは、インフラストラクチャのワークロードはインフラストラクチャノード上で実行されますが、他のワークロードがインフラストラクチャノード上で実行されることは特に禁止されていないことを意味します。言い換えれば、すべてのOperatorに taint/toleration が完全に実装されるまでは、ユーザワークロードとインフラストラクチャワークロードが混在する可能性があります。

taint/tolerationの使用は、これらの演習ではカバーされていません。
====

これを実現するために、`MachineSets` を追加で作成します。

`MachineSets` がどのように動作するかを理解するために、手順を進めていきましょう。

[.console-input]
[source,sh]
----
CLUSTERNAME=$(oc get  infrastructures.config.openshift.io cluster  -o jsonpath='{.status.infrastructureName}')
ZONENAME=$(oc get nodes -L topology.kubernetes.io/zone  --no-headers  | awk '{print $NF}' | tail -1)
oc get machineset -n openshift-machine-api -o yaml ${CLUSTERNAME}-worker-${ZONENAME}
----

#### Metadata
`MachineSet`  の `metadata` には、`MachineSet` の名前や、様々なラベルのような情報が含まれています。


```YAML
metadata:
  annotations:
    machine.openshift.io/GPU: "0"
    machine.openshift.io/memoryMb: "65536"
    machine.openshift.io/vCPU: "16"
  creationTimestamp: "2023-06-19T03:17:34Z"
  generation: 3
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
  name: cluster-zbwlg-cbgq7-worker-us-east-2a
  namespace: openshift-machine-api
  resourceVersion: "95996"
  uid: 73341b44-0495-46c5-b6a2-90aa2650de3c
```

[Note]
====
`MachineAutoScaler` が定義されている `MachineSet` をダンプした場合、`MachineSet` に `annotation` が表示されるかもしれません。
====

#### Selector
`MachineSet` は `Machine` の作成方法を定義し、`Selector` はどのマシンがそのセットに関連付けられているかをOperatorに指示します。

```YAML
spec:
  replicas: 3
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
      machine.openshift.io/cluster-api-machineset: cluster-zbwlg-cbgq7-worker-us-east-2a
```

この場合、クラスタ名は `cluster-zbwlg-cbgq7` であり、セット全体のラベルが追加されています。

### Template Metadata
`template` は、`MachineSet` の一部で、`Machine` をテンプレート化するものです。

```YAML
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-zbwlg-cbgq7-worker-us-east-2a
```

#### Template Spec
`template` は、`Machine`/`Node` をどのように作成するかを指定する必要があります。
`spec`、より具体的には、`providerSpec` には、`Machine` を正しく作成してブートストラップするための重要なAWSデータがすべて含まれていることに気づくでしょう。

この例では、結果として得られるノードが1つ以上の特定のラベルを継承していることを確認したいと思います。上の例で見たように、ラベルは `metadata` セクションにあります。
```YAML
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          ami:
            id: ami-01af87a6ecc18023d
...
```

デフォルトでは、インストーラが作成する `MachineSets` は、ノードに追加のラベルを適用しません。

### カスタムMachineSetの定義
既存の `MachineSet` を分析したところで、次は作成のルールを確認してみましょう。

1. `providerSpec` の中では何も変更しない
2. `machine.openshift.io/cluster-api-cluster: <clusterid>` のインスタンスを変更しない
3. `MachineSet` にユニークな `name` を指定する
4. `machine.openshift.io/cluster-api-machineset` のインスタンスが `name` と一致することを確認する
5. ノードに必要なラベルを `.spec.template.spec.metadata.labels` に追加する
6. `MachineSet` `name` の参照を変更する場合でも、`subnet` を変更しないように注意する

一見複雑に見えますが、以下のように実行してみましょう。

WARNING: 踏み台ホストにログインしている場合は、ログアウトして実行してください。


[.console-input]
[source,sh]
----
bash {{ HOME_PATH }}/support/machineset-generator.sh 1 infra 0 | oc create -f -
----


以下を置換してください。
[.console-input]
[source,sh]
----
export AMI=$(oc get machineset -n openshift-machine-api -l 'machine.openshift.io/os-id!=Windows' -o jsonpath='{.items[0].spec.template.spec.providerSpec.value.ami.id}')
export CLUSTERID=$(oc get infrastructures.config.openshift.io cluster -o jsonpath='{.status.infrastructureName}')
export REGION=$(oc get infrastructures.config.openshift.io cluster -o jsonpath='{.status.platformStatus.aws.region}')
----
cluster-zbwlg-cbgq7　-> $CLUSTERID
us-east-2 -> $REGION
ami-01af87a6ecc18023d -> $AMI

[.console-input]
[source,sh]
----
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
    machine.openshift.io/cluster-api-machine-role: infra
    machine.openshift.io/cluster-api-machine-type: infra
  name: cluster-zbwlg-cbgq7-infra-us-east-2a
  namespace: openshift-machine-api
spec:
  replicas: 0
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
      machine.openshift.io/cluster-api-machineset: cluster-zbwlg-cbgq7-infra-us-east-2a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-zbwlg-cbgq7
        machine.openshift.io/cluster-api-machine-role: infra
        machine.openshift.io/cluster-api-machine-type: infra
        machine.openshift.io/cluster-api-machineset: cluster-zbwlg-cbgq7-infra-us-east-2a
    spec:
      metadata:
        creationTimestamp: null
        labels:
          role: storage-node
          node-role.kubernetes.io/worker: ""
      providerSpec:
        value:
          ami:
            id: ami-01af87a6ecc18023d
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp3
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: cluster-zbwlg-cbgq7-worker-profile
          instanceType: m5.4xlarge
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2a
            region: us-east-2
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - cluster-zbwlg-cbgq7-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - cluster-zbwlg-cbgq7-private-us-east-2a
          tags:
          - name: kubernetes.io/cluster/cluster-zbwlg-cbgq7
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
    kubelet: ""
----




[.console-input]
[source,sh]
----

export MACHINESET=$(oc get machineset -n openshift-machine-api -l machine.openshift.io/cluster-api-machine-role=infra -o jsonpath='{.items[0].metadata.name}')

oc patch machineset $MACHINESET -n openshift-machine-api --type='json' -p='[{"op": "add", "path": "/spec/template/spec/metadata/labels", "value":{"node-role.kubernetes.io/worker":"", "node-role.kubernetes.io/infra":""} }]'

oc scale machineset $MACHINESET -n openshift-machine-api --replicas=1
----



次のように実行します。

[.console-input]
[source,sh]
----
oc get machineset -n openshift-machine-api
----

新しいインフラセットが以下例に似た名前で表示されているはずです。

```
NAME                                    DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-zbwlg-cbgq7-infra-us-east-2a    2         2         1       1           49m
cluster-zbwlg-cbgq7-worker-us-east-2a   3         3         3       3           3d6h
```

まだインスタンスが起動していてブートストラップを行っているため、セットの中には利用可能なマシンがありません。
インスタンスがいつ実行されるかは `oc get machine -n openshift-machine-api` で確認することができます。
次に `oc get node` を使って、実際のノードがいつ結合されて準備が整ったかを確認することができます。

[Note]
====
`Machine` が準備されて `Node` として追加されるまでには数分かかることがあります。
====

[.console-input]
[source,sh]
----
oc get nodes
----

```
NAME                                         STATUS   ROLES                  AGE    VERSION
ip-10-0-148-190.us-east-2.compute.internal   Ready    control-plane,master   3d6h   v1.25.7+eab9cc9
ip-10-0-153-230.us-east-2.compute.internal   Ready    worker                 3d1h   v1.25.7+eab9cc9
ip-10-0-154-47.us-east-2.compute.internal    Ready    infra,worker           95m    v1.25.7+eab9cc9
ip-10-0-190-107.us-east-2.compute.internal   Ready    worker                 3d5h   v1.25.7+eab9cc9
ip-10-0-239-166.us-east-2.compute.internal   Ready    worker                 49m    v1.25.7+eab9cc9
```

どのノードが新しいノードなのか分からなくて困っている場合は、`AGE` カラムを見てみてください。
また、`ROLES` 列では、新しいノードが `worker` と `infra` の両方のロールを持っていることに気づくでしょう。

[.console-input]
[source,sh]
----
oc get nodes -l node-role.kubernetes.io/infra
----

### ラベルを確認する
この例では、一番若いノードは `ip-10-0-133-134.us-east-2.compute.internal` という名前でした。

[.console-input]
[source,sh]
----
YOUNG_INFRA_NODE=$(oc get nodes -l node-role.kubernetes.io/infra  --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[0].metadata.name}')
oc get nodes ${YOUNG_INFRA_NODE} --show-labels | grep --color node-role
----

そして、`LABELS` の欄には、次のように書かれています。

```
beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m5.2xlarge,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/arch=amd64,kubernetes.io/hostname=ip-10-0-140-3,kubernetes.io/os=linux,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos
```

`node-role.kubernetes.io/infra` ラベルが確認できます。

### MachineSetの追加(スケール)
現実的な本番環境では、インフラストラクチャコンポーネントを保持するために、少なくとも3つの `MachineSets` が必要になります。ロギングアグリゲーションソリューションとサービスメッシュの両方がElasticSearchをデプロイするので、ElasticSearchは3つのノードに分散した3つのインスタンスを必要とします。なぜ3つの `MachineSets` が必要なのかですが、理論的には、異なるAZに複数の `MachineSets` を配置することで、AWSがAZを失った場合であっても完全にダウンすることを防ぐためです。

### machine-api-controllers のログ
`openshift-machine-api` プロジェクトにはいくつかの `Pods` があります。そのうちの一つは `machine-api-controllers-56bdc6874f-86jnb` のような名前です。その `Pod` のコンテナ上で `oc log` を使うと、ノードを実際に生成するためのさまざまな演算子のビットを見ることができます。

## Operator について
Operatorはただの `Pods` です。しかし 彼らは特別な `Pods` であり、Kubernetes環境でアプリケーションをデプロイして管理する方法を理解しているソフトウェアです。Operatorのパワーは、`CustomResourceDefinitions` (`CRD`)と呼ばれるKubernetesの機能に依存しています。`CRD` はまさにその名の通りの機能です。これらはカスタムリソースを定義する方法であり、本質的にはKubernetes APIを新しいオブジェクトで拡張するものです。

Kubernetesで `Foo` オブジェクトを作成/読み込み/更新/削除できるようにしたい場合、`Foo` リソースとは何か、どのように動作するのかを定義した `CRD` を作成します。そして、`CRD` のインスタンスである `CustomResources` (`CRs`) を作成することができます。

Operator の場合、一般的なパターンとしては、Operator が `CRs` を見て設定を行い、Kubernetes 環境上で _operate_ を行い、設定で指定されたことを実行するというものです。ここでは、OpenShiftのインフラストラクチャオペレータのいくつかがどのように動作するかを見てみましょう。

## インフラストラクチャコンポーネントの移動
これで特別なノードができたので、インフラストラクチャのコンポーネントをその上に移動させることができます。

### ルータ
OpenShift ルータは `openshift-ingress-operator` という `Operator` によって管理されています。その `Pod` は `openshift-ingress-operator` プロジェクトに存在します。

[.console-input]
[source,sh]
----
oc get pod -n openshift-ingress-operator
----

実際のデフォルトのルータのインスタンスは `openshift-ingress` プロジェクトにあります。 `Pods` を見てみましょう。

[.console-input]
[source,sh]
----
oc get pods -n openshift-ingress -o wide
----

以下のように確認できます。

```
NAME                              READY   STATUS    RESTARTS   AGE     IP            NODE                                         NOMINATED NODE   READINESS GATES
router-default-6475c64c48-7q8kz   1/1     Running   1          6h30m   10.130.0.7    ip-10-0-154-47.us-east-2.compute.internal    <none>           <none>
router-default-6475c64c48-lb7bl   1/1     Running   4          3d6h    10.128.2.10   ip-10-0-153-230.us-east-2.compute.internal   <none>           <none>
```

ルータが動作している `Node` を確認します。

[.console-input]
[source,sh]
----
ROUTER_POD_NODE=$(oc get pods -n openshift-ingress -o jsonpath='{.items[0].spec.nodeName}')
oc get node ${ROUTER_POD_NODE}
----

`worker` の役割が指定されていることが確認できます。

```
NAME                                        STATUS   ROLES    AGE   VERSION
ip-10-0-144-70.us-east-2.compute.internal   Ready    worker   9h    v1.12.4+509916ce1
```

ルータオペレータのデフォルトの設定では、`worker` の役割を持つノードを見つけてルータを配置するようになっています。しかし、専用のインフラストラクチャノードを作成したので、ルータインスタンスを `infra` の役割を持つノードに配置するようにオペレータに指示します。

OpenShiftのルーターオペレータは、`ingresses.config.openshift.io`という`CustomResourceDefinitions`(`CRD`)を使用して、クラスタのデフォルトルーティングサブドメインを定義します。

[.console-input]
[source,sh]
----
oc get ingresses.config.openshift.io cluster -o yaml
----

`cluster` オブジェクトはmasterだけでなくルータオペレータにも観測されます。以下のようなyamlになるでしょう。

```YAML
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2023-06-19T03:16:35Z"
  generation: 1
  name: cluster
  resourceVersion: "112433"
  uid: c2c87e42-2d8e-46c7-82ff-05119d877697
spec:
  domain: apps.cluster-zbwlg.zbwlg.sandbox478.opentlc.com
```

個々のルータのデプロイは `ingresscontrollers.operator.openshift.io` CRD で管理されます。
名前空間 `openshift-ingress-operator` に作成されたデフォルトのものがあります。


[.console-input]
[source,sh]
----
oc get ingresscontrollers.operator.openshift.io default -n openshift-ingress-operator -o yaml
----

以下のようになります。

```YAML
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2023-06-19T03:20:37Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "131344"
  uid: c4e657a3-d449-484a-8267-2d2692a7aba1
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2023-06-19T03:20:39Z"
    reason: Valid
    status: "True"
    type: Admitted
  - lastTransitionTime: "2023-06-19T03:34:49Z"
    status: "True"
    type: PodsScheduled
  ...
  domain: apps.cluster-zbwlg.zbwlg.sandbox478.opentlc.com
  endpointPublishingStrategy:
    loadBalancer:
      dnsManagementPolicy: Managed
      providerParameters:
        aws:
          classicLoadBalancer:
            connectionIdleTimeout: 0s
          type: Classic
        type: AWS
      scope: External
    type: LoadBalancerService
  observedGeneration: 2
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
```

ルータPodがインフラストラクチャノードにヒットするように指示する `nodeSelector` を指定するには、以下の設定を適用します。
[.console-input]
[source,sh]
----
vi ingresscontroller.yaml
----

[.console-input]
[source,sh]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  name: default
  namespace: openshift-ingress-operator
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
----

[.console-input]
[source,sh]
----
oc apply -f ingresscontroller.yaml
----

実行：
[.console-input]
[source,sh]
----
oc get pod -n openshift-ingress -o wide
----

[Note]
====
ルーターの移動中にセッションがタイムアウトすることがあります。
ページを更新してセッションを取り戻してください。
端末セッションが失われることはありませんが、手動でこのページに戻る必要があるかもしれません。
====

もし十分に手際が良ければ、`Terminating` か `ContainerCreating` のいずれかのPodを捕まえることができるかもしれません。
`Terminating` Podはワーカーノードの1つで動作していました。
実行中の `Running` Podは最終的に `infra` ロールを持つノードの1つで動作しています。
